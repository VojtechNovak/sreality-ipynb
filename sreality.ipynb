{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd                     \n",
    "from pandas import DataFrame           \n",
    "import numpy as np                      \n",
    "\n",
    "from itertools import chain\n",
    "\n",
    "from collections import Counter         \n",
    "from datetime import datetime           \n",
    "import re                              \n",
    "from time import sleep                  \n",
    "import random                           \n",
    "import math                            \n",
    "import time                             \n",
    "import itertools                       \n",
    "import sys\n",
    "import openpyxl\n",
    "\n",
    "\n",
    "import requests                        \n",
    "from bs4 import BeautifulSoup           \n",
    "from selenium import webdriver         \n",
    "import json                            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_soup_elements(typ_obchodu = \"prodej\", typ_stavby = \"byty\", pages = 1):  \n",
    "    \n",
    "    browser = webdriver.Chrome()\n",
    "    \n",
    "    \n",
    "    url_x = r\"https://www.sreality.cz/hledani\"             \n",
    "    url = url_x + \"/\" +  typ_obchodu + \"/\" +  typ_stavby\n",
    "\n",
    "    browser.get(url)    # (url).text ??\n",
    "    sleep(random.uniform(1.0, 1.5))\n",
    "    innerHTML = browser.execute_script(\"return document.body.innerHTML\")\n",
    "    soup = BeautifulSoup(innerHTML,'lxml') # \"parser\" ??\n",
    "    \n",
    "    elements = []    \n",
    "    \n",
    "    for link in soup.findAll('a', attrs={'href': re.compile(\"^/detail/\")}):      \n",
    "        link = link.get('href')   \n",
    "        elements.append(link)     \n",
    "    elements = elements[0::2]   \n",
    "\n",
    "    \n",
    "    records = soup.find_all(class_ ='numero ng-binding')[1].text\n",
    "    records = re.split(r'\\D', str(records))                         \n",
    "    records = \",\".join(records).replace(\",\", \"\")\n",
    "    records = int(records)\n",
    "    max_page = math.ceil(records / 20)   \n",
    "    print(\"----------------\")\n",
    "    print(\"Scrapuji: \" + str(typ_obchodu) + \" \" + str(typ_stavby))\n",
    "    print(\"Celkem inzerátů: \" + str(records))\n",
    "    print(\"Celkem stránek: \" + str(max_page))\n",
    "    \n",
    "    if pages == 999:\n",
    "        print(\"Scrapuji (pouze) \" + str(pages) + \" stran.\")\n",
    "        print(\"----------------\")\n",
    "  \n",
    "    \n",
    "    for i in range(pages-1):   \n",
    "        i = i+2\n",
    "        \n",
    "        sys.stdout.write('\\r'+ \"Strana \" + str(i-1) + \" = \" + str(round(100*(i-1)/(pages), 2)) + \"% progress. Zbývá cca: \" + str(round(random.uniform(3.4, 3.8)*(pages-(i-1)), 2 )) + \" sekund.\")    # Asi upravím čas, na rychlejším kabelu v obýváku je to občas i tak 3 sec :O\n",
    "\n",
    "        url2 = url + \"?strana=\" + str(i)\n",
    "        browser.get(url2)\n",
    "\n",
    "        sleep(random.uniform(1.0, 1.5))\n",
    "\n",
    "        innerHTML = browser.execute_script(\"return document.body.innerHTML\")\n",
    "        soup2 = BeautifulSoup(innerHTML,'lxml') \n",
    "        \n",
    "        elements2 = []\n",
    "        \n",
    "        for link in soup2.findAll('a', attrs={'href': re.compile(\"^/detail/prodej/\")}):  \n",
    "            link = link.get('href') \n",
    "            elements2.append(link)  \n",
    "   \n",
    "        elements2 = elements2[0::2]  \n",
    "        \n",
    "        elements = elements + elements2     \n",
    "\n",
    "    \n",
    "    browser.quit()   \n",
    "    \n",
    "    return elements\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_img(typ_obchodu = \"prodej\", typ_stavby = \"byty\", pages = 1):  \n",
    "    \n",
    "    browser = webdriver.Chrome()\n",
    "    \n",
    "    \n",
    "    url_x = r\"https://www.sreality.cz/hledani\"             \n",
    "    url = url_x + \"/\" +  typ_obchodu + \"/\" +  typ_stavby\n",
    "\n",
    "    browser.get(url)    # (url).text ??\n",
    "    sleep(random.uniform(1.0, 1.5))\n",
    "    innerHTML = browser.execute_script(\"return document.body.innerHTML\")\n",
    "    soup = BeautifulSoup(innerHTML,'lxml') # \"parser\" ?? \n",
    "\n",
    "    img = []\n",
    "\n",
    "    for link in soup.findAll('img', attrs={'src': re.compile(\"^https://d18\")}):      \n",
    "        link = link.get('src')   \n",
    "        img.append(link)     \n",
    "    #del img[0]\n",
    "    img = img[0::6] \n",
    "\n",
    "    \n",
    "    records = soup.find_all(class_ ='numero ng-binding')[1].text\n",
    "    records = re.split(r'\\D', str(records))                         \n",
    "    records = \",\".join(records).replace(\",\", \"\")\n",
    "    records = int(records)\n",
    "    max_page = math.ceil(records / 20)   \n",
    "    print(\"----------------\")\n",
    "    print(\"Scrapuji: \" + str(typ_obchodu) + \" \" + str(typ_stavby))\n",
    "    print(\"Celkem inzerátů: \" + str(records))\n",
    "    print(\"Celkem stránek: \" + str(max_page))\n",
    "    \n",
    "    if pages == 999:\n",
    "        print(\"Scrapuji (pouze) \" + str(pages) + \" stran.\")\n",
    "        print(\"----------------\")\n",
    "  \n",
    "    \n",
    "    for i in range(pages-1):   \n",
    "        i = i+2\n",
    "        \n",
    "        sys.stdout.write('\\r'+ \"Strana \" + str(i-1) + \" = \" + str(round(100*(i-1)/(pages), 2)) + \"% progress. Zbývá cca: \" + str(round(random.uniform(3.4, 3.8)*(pages-(i-1)), 2 )) + \" sekund.\")    # Asi upravím čas, na rychlejším kabelu v obýváku je to občas i tak 3 sec :O\n",
    "\n",
    "        url2 = url + \"?strana=\" + str(i)\n",
    "        browser.get(url2)\n",
    "\n",
    "        sleep(random.uniform(1.0, 1.5))\n",
    "\n",
    "        innerHTML = browser.execute_script(\"return document.body.innerHTML\")\n",
    "        soup2 = BeautifulSoup(innerHTML,'lxml')  \n",
    "\n",
    "        img2 = []\n",
    "\n",
    "        for link in soup.findAll('img', attrs={'src': re.compile(\"^https://d18\")}):      \n",
    "            link = link.get('src')   \n",
    "            img2.append(link)     \n",
    "        #del img[0]\n",
    "        img2 = img2[0::6] \n",
    "        img = img + img2\n",
    "\n",
    "    \n",
    "    browser.quit()   \n",
    "    \n",
    "    return img\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def elements_and_ids(x):\n",
    "    \n",
    "    elements = pd.DataFrame({\"url\":x})\n",
    "\n",
    "    def get_id(x):\n",
    "        x = x.split(\"/\")[-1]\n",
    "        return x\n",
    "    \n",
    "    len1 = len(elements)\n",
    "    elements = elements.drop_duplicates(subset = [ \"url\"], keep = \"first\", inplace = False)   \n",
    "    len2 = len(elements)                                                                             \n",
    "                                                                                                      \n",
    "    print(\"-- Vymazáno \" + str(len1-len2) + \" záznamů kvůli duplikaci.\")\n",
    "    return elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrap_all(typ_obchodu = \"prodej\", typ_stavby = \"byty\", pages = 1):\n",
    "    \n",
    "    data = get_soup_elements(typ_obchodu = typ_obchodu, typ_stavby = typ_stavby, pages = pages)\n",
    "    print( \"1/2 Data scrapnuta, získávám URLs.\")\n",
    "    \n",
    "    data = elements_and_ids(data)\n",
    "    data.to_excel(r\"a1_URLs_prodej_byty.xlsx\")\n",
    "    print( \"2/2 Získány URL, nyní získávám imgs - několik minut...\")\n",
    "    img1 = get_img(typ_obchodu = typ_obchodu, typ_stavby = typ_stavby, pages = pages)\n",
    "    l = len(img1)\n",
    "    data = data.head(l)\n",
    "    data[\"img\"] = img1\n",
    "    \n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------\n",
      "Scrapuji: prodej byty\n",
      "Celkem inzerátů: 20505\n",
      "Celkem stránek: 1026\n",
      "Strana 9 = 90.0% progress. Zbývá cca: 3.58 sekund..1/2 Data scrapnuta, získávám URLs.\n",
      "-- Vymazáno 496 záznamů kvůli duplikaci.\n",
      "2/2 Získány URL, nyní získávám imgs - několik minut...\n",
      "----------------\n",
      "Scrapuji: prodej byty\n",
      "Celkem inzerátů: 20397\n",
      "Celkem stránek: 1020\n",
      "Strana 9 = 90.0% progress. Zbývá cca: 3.59 sekund.."
     ]
    }
   ],
   "source": [
    "data = scrap_all(pages=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[['null','detail', 'prodej','byt','velikost','lokace','id']] = data.url.str.split(\"/\", expand = True)\n",
    "data = data.drop(['url','null', 'detail','prodej','byt','id'], axis=1)\n",
    "\n",
    "data.to_csv('sreality.csv', sep=';',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "conn = psycopg2.connect(host='localhost',\n",
    "port = '5432',\n",
    "user = 'postgres',\n",
    "password = '159357lol',\n",
    "dbname = 'postgres')\n",
    "\n",
    "cur = conn.cursor()\n",
    "\n",
    "cur.execute('DROP TABLE IF EXISTS sreality')\n",
    "\n",
    "create_script = \"CREATE TABLE IF NOT EXISTS sreality (IMG VARCHAR(255), Velikost VARCHAR(255), Lokace VARCHAR(255))\"\n",
    "cur.execute(create_script)\n",
    "\n",
    "with open('sreality.csv', 'r') as f:\n",
    "    next(f)\n",
    "    cur.copy_from(f, 'sreality', sep=';', columns=('img', 'velikost', 'lokace'))\n",
    "\n",
    "\n",
    "conn.commit()\n",
    "conn.close()\n",
    "cur.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app '__main__'\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\n",
      " * Running on http://127.0.0.1:5000\n",
      "Press CTRL+C to quit\n",
      "[2023-07-09 01:25:33,197] ERROR in app: Exception on / [GET]\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\petre\\anaconda3\\envs\\myenv\\lib\\site-packages\\flask\\app.py\", line 2190, in wsgi_app\n",
      "    response = self.full_dispatch_request()\n",
      "  File \"c:\\Users\\petre\\anaconda3\\envs\\myenv\\lib\\site-packages\\flask\\app.py\", line 1486, in full_dispatch_request\n",
      "    rv = self.handle_user_exception(e)\n",
      "  File \"c:\\Users\\petre\\anaconda3\\envs\\myenv\\lib\\site-packages\\flask\\app.py\", line 1484, in full_dispatch_request\n",
      "    rv = self.dispatch_request()\n",
      "  File \"c:\\Users\\petre\\anaconda3\\envs\\myenv\\lib\\site-packages\\flask\\app.py\", line 1469, in dispatch_request\n",
      "    return self.ensure_sync(self.view_functions[rule.endpoint])(**view_args)\n",
      "  File \"C:\\Users\\petre\\AppData\\Local\\Temp\\ipykernel_9524\\1087673341.py\", line 20, in index\n",
      "    rows = cur.fetchall()\n",
      "psycopg2.InterfaceError: cursor already closed\n",
      "127.0.0.1 - - [09/Jul/2023 01:25:33] \"GET / HTTP/1.1\" 500 -\n",
      "[2023-07-09 01:25:33,257] ERROR in app: Exception on / [GET]\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\petre\\anaconda3\\envs\\myenv\\lib\\site-packages\\flask\\app.py\", line 2190, in wsgi_app\n",
      "    response = self.full_dispatch_request()\n",
      "  File \"c:\\Users\\petre\\anaconda3\\envs\\myenv\\lib\\site-packages\\flask\\app.py\", line 1486, in full_dispatch_request\n",
      "    rv = self.handle_user_exception(e)\n",
      "  File \"c:\\Users\\petre\\anaconda3\\envs\\myenv\\lib\\site-packages\\flask\\app.py\", line 1484, in full_dispatch_request\n",
      "    rv = self.dispatch_request()\n",
      "  File \"c:\\Users\\petre\\anaconda3\\envs\\myenv\\lib\\site-packages\\flask\\app.py\", line 1469, in dispatch_request\n",
      "    return self.ensure_sync(self.view_functions[rule.endpoint])(**view_args)\n",
      "  File \"C:\\Users\\petre\\AppData\\Local\\Temp\\ipykernel_9524\\1087673341.py\", line 20, in index\n",
      "    rows = cur.fetchall()\n",
      "psycopg2.InterfaceError: cursor already closed\n",
      "127.0.0.1 - - [09/Jul/2023 01:25:33] \"GET / HTTP/1.1\" 500 -\n",
      "127.0.0.1 - - [09/Jul/2023 01:25:33] \"GET /favicon.ico HTTP/1.1\" 404 -\n",
      "[2023-07-09 01:26:17,283] ERROR in app: Exception on / [GET]\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\petre\\anaconda3\\envs\\myenv\\lib\\site-packages\\flask\\app.py\", line 2190, in wsgi_app\n",
      "    response = self.full_dispatch_request()\n",
      "  File \"c:\\Users\\petre\\anaconda3\\envs\\myenv\\lib\\site-packages\\flask\\app.py\", line 1486, in full_dispatch_request\n",
      "    rv = self.handle_user_exception(e)\n",
      "  File \"c:\\Users\\petre\\anaconda3\\envs\\myenv\\lib\\site-packages\\flask\\app.py\", line 1484, in full_dispatch_request\n",
      "    rv = self.dispatch_request()\n",
      "  File \"c:\\Users\\petre\\anaconda3\\envs\\myenv\\lib\\site-packages\\flask\\app.py\", line 1469, in dispatch_request\n",
      "    return self.ensure_sync(self.view_functions[rule.endpoint])(**view_args)\n",
      "  File \"C:\\Users\\petre\\AppData\\Local\\Temp\\ipykernel_9524\\1087673341.py\", line 20, in index\n",
      "    rows = cur.fetchall()\n",
      "psycopg2.InterfaceError: cursor already closed\n",
      "127.0.0.1 - - [09/Jul/2023 01:26:17] \"GET / HTTP/1.1\" 500 -\n"
     ]
    }
   ],
   "source": [
    "from flask import Flask, render_template\n",
    "import psycopg2\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "@app.route('/')\n",
    "def index():\n",
    "    conn = psycopg2.connect(host='localhost',\n",
    "port = '5432',\n",
    "user = 'postgres',\n",
    "password = '159357lol',\n",
    "dbname = 'postgres')\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    cursor.execute(\"SELECT IMG , Velikost, Lokace FROM sreality LIMIT 500\")\n",
    "    rows = cursor.fetchall()\n",
    "\n",
    "    # Fetch all rows from the result\n",
    "    rows = cur.fetchall()\n",
    "\n",
    "    # Generate HTML for the table\n",
    "    html = \"<html><body><table>\"\n",
    "    html += \"<tr><th>Velikost</th><th>Lokace</th><th>Image</th></tr>\"\n",
    "\n",
    "    for row in rows:\n",
    "        html += \"<tr>\"\n",
    "        html += \"<td>{}</td>\".format(row[0])\n",
    "        html += \"<td>{}</td>\".format(row[1])\n",
    "        html += \"<td><img src='{}' width='400' height='300'></td>\".format(row[3])\n",
    "        html += \"</tr>\"\n",
    "\n",
    "    html += \"</table></body></html>\"\n",
    "\n",
    "    # Close the database connection\n",
    "    cur.close()\n",
    "    conn.close()\n",
    "    \n",
    "    display(HTML(table))\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
